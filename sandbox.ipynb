{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 14,
>>>>>>> pushforward
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import dataloader, generate, plot_1d_statistic_over_time, plot_loss\n",
    "from models import Forward, Posterior, Prior, Decoder\n",
<<<<<<< HEAD
    "from train import run, val_step\n",
    "import os, pickle, random, shutil\n",
=======
    "from train_2 import run\n",
    "import os, pickle\n",
    "import pandas as pd \n",
    "import torch \n",
    "from torch.utils.data import Subset\n",
>>>>>>> pushforward
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "Make sure all data files are stored somewhere in `\"./data/\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(root, file) for root, _, files in os.walk(\"./data/\") for file in files if file.endswith(\".npz\")]\n",
    "random.seed(42)\n",
    "random.shuffle(files)\n",
    "\n",
    "train_files = files[:int(0.6 * len(files))]\n",
    "val_files = files[int(0.6 * len(files)):int(0.8 * len(files))]\n",
    "test_files = files[int(0.8 * len(files)):]\n",
    "\n",
    "os.makedirs(\"./data/train\", exist_ok=True)\n",
    "os.makedirs(\"./data/val\", exist_ok=True)\n",
    "os.makedirs(\"./data/test\", exist_ok=True)\n",
    "\n",
    "for f in train_files:\n",
    "    if not f.split(\"/\")[-1] in os.listdir(\"./data/train\"):\n",
    "        shutil.move(f, \"./data/train\")\n",
    "for f in val_files:\n",
    "    if not f.split(\"/\")[-1] in os.listdir(\"./data/val\"):\n",
    "        shutil.move(f, \"./data/val\")\n",
    "for f in test_files:\n",
    "    if not f.split(\"/\")[-1] in os.listdir(\"./data/test\"):\n",
    "        shutil.move(f, \"./data/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B input parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/NuclearFusion/lib/python3.11/site-packages/keras/src/backend/common/backend_utils.py:91: UserWarning: You might experience inconsistencies across backends when calling conv transpose with kernel_size=3, stride=2, dilation_rate=1, padding=same, output_padding=1.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/10 [02:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/basic0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m x_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m500\u001b[39m, steps\u001b[38;5;241m=\u001b[39mbatch_size, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_tplus1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_field\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate by generating multiple trajectories from one random starting point\u001b[39;00m\n\u001b[1;32m     29\u001b[0m trajectory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_ds))\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Master TUe/Y2/Q1/2AMM40 Adv. Topics in AI/TokamakSimulation/TokamakSimulation/train_2.py:111\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(train_loader, val_loader, forward_t, forward_tplus1, prior, posterior, decoder, optimizer, b_field, x_tensor, save_dir, max_epochs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x_t_hat \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconcatenate([x_t_hat, x_t[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m x_t_hat, kl_loss, rec_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tplus1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_tplus1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Just a fancy way to append the mean\u001b[39;00m\n\u001b[1;32m    113\u001b[0m train_loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m][i] \u001b[38;5;241m=\u001b[39m train_loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mj) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mj\u001b[38;5;241m*\u001b[39mkl_loss\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Master TUe/Y2/Q1/2AMM40 Adv. Topics in AI/TokamakSimulation/TokamakSimulation/train_2.py:63\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x_t, x_tplus1, forward_t, forward_tplus1, prior, posterior, decoder, opt, x_tensor, b_field)\u001b[0m\n\u001b[1;32m     58\u001b[0m _, \u001b[38;5;241m*\u001b[39mmu_logvar \u001b[38;5;241m=\u001b[39m prior(h_t)\n\u001b[1;32m     59\u001b[0m kl_nll \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m     60\u001b[0m     log_normal_diag(z, mu, logvar) \u001b[38;5;241m-\u001b[39m log_normal_diag(z, \u001b[38;5;241m*\u001b[39mmu_logvar), \n\u001b[1;32m     61\u001b[0m     axis\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# sum reduction\u001b[39;00m\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m rec_ll \u001b[38;5;241m=\u001b[39m \u001b[43mlog_bernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tplus1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tplus1_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# pde_loss_value = pde_loss(x_tplus1_hat, x_tplus1, b_field, x_tensor)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39mrec_ll \u001b[38;5;241m+\u001b[39m kl_nll)  \u001b[38;5;66;03m# mean reduction\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Master TUe/Y2/Q1/2AMM40 Adv. Topics in AI/TokamakSimulation/TokamakSimulation/train_2.py:13\u001b[0m, in \u001b[0;36mlog_bernoulli\u001b[0;34m(x, p)\u001b[0m\n\u001b[1;32m     11\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.e-5\u001b[39m\n\u001b[1;32m     12\u001b[0m pp \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mclip(p, eps, \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m eps)\n\u001b[0;32m---> 13\u001b[0m log_p \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m x) \u001b[38;5;241m*\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m pp)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msum(log_p, [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# Instantiate models on the chosen device\n",
    "forward_t = Forward().to(device)\n",
    "forward_tplus1 = Forward().to(device)\n",
    "prior = Prior().to(device)\n",
    "posterior = Posterior().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "# Instantiate optimizer\n",
    "opt = keras.optimizers.AdamW(1e-4)\n",
    "\n",
    "#B Input parameter \n",
    "b_field = pd.read_csv('b-field.csv', delimiter=',', index_col=0)\n",
    "b_field_values = b_field.values.flatten()  # Convert to a 1D Numpy array\n",
    "b_field = torch.tensor(b_field_values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Get data\n",
<<<<<<< HEAD
    "train_loader = dataloader(data_dir=\"./data/train\", batch_size=32)\n",
    "val_loader = dataloader(data_dir=\"./data/val\", batch_size=32)\n",
    "test_loader = dataloader(data_dir=\"./data/test\", batch_size=32)\n",
    "\n",
    "# Run training\n",
    "save_dir = \"./results/basic-standardscale\"\n",
    "run(train_loader, val_loader, forward_t, forward_tplus1, prior, posterior, decoder, opt, save_dir, 200, 10)\n",
=======
    "batch_size = 8\n",
    "train_loader = TrajectoryPreservingSampler(FusionDataset(data_dir=\"./data/train\",max_instances=30), batch_size=batch_size)\n",
    "val_loader = TrajectoryPreservingSampler(FusionDataset(data_dir=\"./data/val\",max_instances=10), batch_size=batch_size)\n",
    "test_ds = TrajectoryPreservingSampler(FusionDataset(data_dir=\"./data/test\"), batch_size=16)\n",
    "\n",
    "# Run training\n",
    "save_dir = \"./results/basic0\"\n",
    "x_tensor = torch.linspace(0, 500, steps=batch_size, requires_grad=True).unsqueeze(1).to(device)\n",
    "\n",
    "run(train_loader, val_loader, forward_t, forward_tplus1, prior, posterior, decoder, opt, b_field, x_tensor, save_dir, 10)\n",
>>>>>>> pushforward
    "\n",
    "# # Plot loss\n",
    "plot_loss(f\"{save_dir}/history.json\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained models\n",
    "forward_t.load_weights(f\"{save_dir}/forward_t.weights.h5\")\n",
    "prior.load_weights(f\"{save_dir}/prior.weights.h5\")\n",
    "decoder.load_weights(f\"{save_dir}/decoder.weights.h5\")\n",
    "\n",
    "# Evaluate by calculating loss over test set\n",
    "test_loss = 0\n",
    "for i, (x_t, x_tplus1) in enumerate(test_loader):\n",
    "    test_loss += val_step(x_t, x_tplus1, forward_t, prior, decoder)\n",
    "test_loss /= i+1\n",
    "print(\"Test reconstruction loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate\n",
    "Make sure the trained models have been loaded with the code cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it for various starting points\n",
    "for j in range(4):\n",
    "    # Select one random ground truth starting point\n",
    "    trajectory = test_loader.dataset.get_trajectory(pushforward=False)\n",
    "    # Generate multiple trajectories from one starting point\n",
    "    trajectory_hats = []\n",
    "    for i in range(6):\n",
    "        # Generate trajectory\n",
    "        trajectory_hat = generate(trajectory, forward_t, prior, decoder)\n",
    "        trajectory_hats.append(keras.ops.concatenate(trajectory_hat))\n",
    "        # Save trajectories as figure\n",
    "        fig = plot_1d_statistic_over_time(trajectory_hats[i].detach().cpu(), 0, \"I don't know what this variable is\");\n",
    "        fig.savefig(f\"{save_dir}/gen_{j}-{i}\")\n",
    "    # Save trajectories as tensors\n",
    "    trajectory_hats = keras.ops.stack(trajectory_hats)\n",
    "    with open(f\"{save_dir}/generated_trajectories{j}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(trajectory_hats, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NuclearFusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
