{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import FusionDataset, TrajectoryPreservingSampler, generate, plot_1d_statistic_over_time\n",
    "from models import Forward, Posterior, Prior, Decoder\n",
    "from train_2 import run\n",
    "import os, pickle\n",
    "import pandas as pd \n",
    "import torch \n",
    "from torch.utils.data import Subset\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B input parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/NuclearFusion/lib/python3.11/site-packages/keras/src/backend/common/backend_utils.py:91: UserWarning: You might experience inconsistencies across backends when calling conv transpose with kernel_size=3, stride=2, dilation_rate=1, padding=same, output_padding=1.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/10 [02:26<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/basic0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m x_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m500\u001b[39m, steps\u001b[38;5;241m=\u001b[39mbatch_size, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_tplus1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_field\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate by generating multiple trajectories from one random starting point\u001b[39;00m\n\u001b[1;32m     29\u001b[0m trajectory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_ds))\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Master TUe/Y2/Q1/2AMM40 Adv. Topics in AI/TokamakSimulation/TokamakSimulation/train_2.py:111\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(train_loader, val_loader, forward_t, forward_tplus1, prior, posterior, decoder, optimizer, b_field, x_tensor, save_dir, max_epochs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x_t_hat \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconcatenate([x_t_hat, x_t[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m x_t_hat, kl_loss, rec_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tplus1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_tplus1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposterior\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Just a fancy way to append the mean\u001b[39;00m\n\u001b[1;32m    113\u001b[0m train_loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m][i] \u001b[38;5;241m=\u001b[39m train_loss_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkl_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mj) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mj\u001b[38;5;241m*\u001b[39mkl_loss\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Master TUe/Y2/Q1/2AMM40 Adv. Topics in AI/TokamakSimulation/TokamakSimulation/train_2.py:63\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(x_t, x_tplus1, forward_t, forward_tplus1, prior, posterior, decoder, opt, x_tensor, b_field)\u001b[0m\n\u001b[1;32m     58\u001b[0m _, \u001b[38;5;241m*\u001b[39mmu_logvar \u001b[38;5;241m=\u001b[39m prior(h_t)\n\u001b[1;32m     59\u001b[0m kl_nll \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m     60\u001b[0m     log_normal_diag(z, mu, logvar) \u001b[38;5;241m-\u001b[39m log_normal_diag(z, \u001b[38;5;241m*\u001b[39mmu_logvar), \n\u001b[1;32m     61\u001b[0m     axis\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# sum reduction\u001b[39;00m\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m rec_ll \u001b[38;5;241m=\u001b[39m \u001b[43mlog_bernoulli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_tplus1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_tplus1_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# pde_loss_value = pde_loss(x_tplus1_hat, x_tplus1, b_field, x_tensor)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39mrec_ll \u001b[38;5;241m+\u001b[39m kl_nll)  \u001b[38;5;66;03m# mean reduction\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Master TUe/Y2/Q1/2AMM40 Adv. Topics in AI/TokamakSimulation/TokamakSimulation/train_2.py:13\u001b[0m, in \u001b[0;36mlog_bernoulli\u001b[0;34m(x, p)\u001b[0m\n\u001b[1;32m     11\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.e-5\u001b[39m\n\u001b[1;32m     12\u001b[0m pp \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mclip(p, eps, \u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m eps)\n\u001b[0;32m---> 13\u001b[0m log_p \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m x) \u001b[38;5;241m*\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m pp)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39msum(log_p, [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, mps:0 and cpu!"
     ]
    }
   ],
   "source": [
    "# Instantiate models on the chosen device\n",
    "forward_t = Forward().to(device)\n",
    "forward_tplus1 = Forward().to(device)\n",
    "prior = Prior().to(device)\n",
    "posterior = Posterior().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "# Instantiate optimizer\n",
    "opt = keras.optimizers.AdamW()\n",
    "\n",
    "#B Input parameter \n",
    "b_field = pd.read_csv('b-field.csv', delimiter=',', index_col=0)\n",
    "b_field_values = b_field.values.flatten()  # Convert to a 1D Numpy array\n",
    "b_field = torch.tensor(b_field_values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Get data\n",
    "batch_size = 8\n",
    "train_loader = TrajectoryPreservingSampler(FusionDataset(data_dir=\"./data/train\",max_instances=30), batch_size=batch_size)\n",
    "val_loader = TrajectoryPreservingSampler(FusionDataset(data_dir=\"./data/val\",max_instances=10), batch_size=batch_size)\n",
    "test_ds = TrajectoryPreservingSampler(FusionDataset(data_dir=\"./data/test\"), batch_size=16)\n",
    "\n",
    "# Run training\n",
    "save_dir = \"./results/basic0\"\n",
    "x_tensor = torch.linspace(0, 500, steps=batch_size, requires_grad=True).unsqueeze(1).to(device)\n",
    "\n",
    "run(train_loader, val_loader, forward_t, forward_tplus1, prior, posterior, decoder, opt, b_field, x_tensor, save_dir, 10)\n",
    "\n",
    "# Evaluate by generating multiple trajectories from one random starting point\n",
    "trajectory = next(iter(test_ds))\n",
    "trajectory_hats = []\n",
    "for i in range(6):\n",
    "    trajectory_hats.append(keras.ops.concatenate(generate(trajectory, forward_t, prior, decoder)))\n",
    "    # Save trajectories as figure\n",
    "    fig = plot_1d_statistic_over_time(trajectory_hats[i].detach().cpu(), 0, \"I don't know what this variable is\");\n",
    "    fig.savefig(f\"{save_dir}/gen_{i}\")\n",
    "# Save trajectories as tensors\n",
    "trajectory_hats = keras.ops.stack(trajectory_hats)\n",
    "with open(f\"{save_dir}/generated_trajectories.pkl\", \"wb\") as file:\n",
    "    pickle.dump(trajectory_hats, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NuclearFusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
