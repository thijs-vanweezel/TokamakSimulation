{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5uQFzk-Ds8H",
        "outputId": "f39caf4a-1da4-4774-9790-b8a5d166a9a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.15.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch_geometric) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hkjWBrQ_6YIE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n",
        "import os\n",
        "import re\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lf5SEf-Dc8p",
        "outputId": "686c087e-94b3-47d8-c273-87b0d382fd30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_data_by_file(folders, base_dir):\n",
        "    # Create dictionaries to store output and forcing data per file\n",
        "    images = {}\n",
        "    idx = 0\n",
        "    for folder in folders:\n",
        "        folder_path = os.path.join(base_dir, folder)\n",
        "        for file in os.listdir(folder_path):\n",
        "            if file.endswith('.npz'):\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "                data = np.load(file_path)\n",
        "                images[idx] = data\n",
        "                idx += 1\n",
        "\n",
        "    return images"
      ],
      "metadata": {
        "id": "UMcu1XUy6cmv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folders = ['50T_ramp_up', '50T_ramp_down']\n",
        "base_dir = '/content/drive/MyDrive/Adv Topics In AI/preprocessed'\n",
        "images = separate_data_by_file(folders, base_dir)"
      ],
      "metadata": {
        "id": "Cpy0TPLm6dVD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class TimeSeriesPerInstanceDataset(Dataset):\n",
        "    def __init__(self, images, normalize=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (dict): A dictionary where each key is an instance and the value is a dictionary with 'output' and 'forcing'.\n",
        "                           Each entry contains multiple time steps of data.\n",
        "            normalize (bool): Whether to apply normalization to the inputs and targets per variable.\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.normalize = normalize\n",
        "\n",
        "        # Precompute all (instance_idx, time_idx) pairs to ensure each item in the dataset\n",
        "        self.index_pairs = []\n",
        "        for key, data in self.images.items():\n",
        "            timesteps_in_instance = data['output'].shape[0] - 1  # -1 because we need t and t+1\n",
        "            for time_idx in range(timesteps_in_instance):\n",
        "                self.index_pairs.append((key, time_idx))\n",
        "\n",
        "        # Compute the min and max for each channel if normalization is enabled\n",
        "        if self.normalize:\n",
        "            self.min_max_values = self.compute_channel_min_max()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index_pairs)\n",
        "\n",
        "    def compute_channel_min_max(self):\n",
        "        \"\"\" Compute the min and max values for each channel (per variable) across all instances. \"\"\"\n",
        "        # Initialize min and max arrays for both input and target channels\n",
        "        input_min, input_max = np.full(8, float('inf')), np.full(8, float('-inf'))  # 8 input channels (6 outputs, 2 forcing)\n",
        "        target_min, target_max = np.full(6, float('inf')), np.full(6, float('-inf'))  # 6 target channels\n",
        "\n",
        "        for _, data in self.images.items():\n",
        "            output = data['output']  # shape (timesteps, 500, 6)\n",
        "            forcing = data['forcing']  # shape (timesteps, 500, 2)\n",
        "\n",
        "            # For each channel in 'output' and 'forcing', update min and max\n",
        "            for i in range(6):  # output has 6 channels\n",
        "                channel_data = output[:, :, i]\n",
        "                input_min[i] = min(input_min[i], np.min(channel_data))\n",
        "                input_max[i] = max(input_max[i], np.max(channel_data))\n",
        "                target_min[i] = min(target_min[i], np.min(channel_data))\n",
        "                target_max[i] = max(target_max[i], np.max(channel_data))\n",
        "\n",
        "            for i in range(2):  # forcing has 2 channels\n",
        "                channel_data = forcing[:, :, i]\n",
        "                input_min[6 + i] = min(input_min[6 + i], np.min(channel_data))\n",
        "                input_max[6 + i] = max(input_max[6 + i], np.max(channel_data))\n",
        "\n",
        "        return {\n",
        "            \"input_min\": input_min, \"input_max\": input_max,\n",
        "            \"target_min\": target_min, \"target_max\": target_max\n",
        "        }\n",
        "\n",
        "    def normalize_per_channel(self, data, min_vals, max_vals):\n",
        "        \"\"\" Normalize each channel separately using Min-Max normalization. \"\"\"\n",
        "        for i in range(data.shape[1]):  # Loop through each channel\n",
        "            data[:, i] = (data[:, i] - min_vals[i]) / (max_vals[i] - min_vals[i] + 1e-6)  # Normalize per channel\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\" Returns normalized input and target. \"\"\"\n",
        "        instance_idx, time_idx = self.index_pairs[idx]\n",
        "        current_data = self.images[instance_idx]\n",
        "\n",
        "        # Extract input (output_t, forcing_{t+1}) and target (output_{t+1})\n",
        "        output_t = current_data['output'][time_idx]  # shape (500, 6)\n",
        "        forcing_t_plus_1 = current_data['forcing'][time_idx + 1]  # shape (500, 2)\n",
        "        target_t_plus_1 = current_data['output'][time_idx + 1]  # shape (500, 6)\n",
        "\n",
        "        # Concatenate output_t and forcing_t_plus_1 to form the input\n",
        "        input_t = np.concatenate((output_t, forcing_t_plus_1), axis=-1)  # shape (500, 8)\n",
        "\n",
        "        # Normalize input and target per channel if normalization is enabled\n",
        "        if self.normalize:\n",
        "            input_t = self.normalize_per_channel(\n",
        "                input_t,\n",
        "                self.min_max_values[\"input_min\"],\n",
        "                self.min_max_values[\"input_max\"]\n",
        "            )\n",
        "            target_t_plus_1 = self.normalize_per_channel(\n",
        "                target_t_plus_1,\n",
        "                self.min_max_values[\"target_min\"],\n",
        "                self.min_max_values[\"target_max\"]\n",
        "            )\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        input_t = torch.tensor(input_t, dtype=torch.float32).permute(1, 0)  # (8, 500)\n",
        "        target_t_plus_1 = torch.tensor(target_t_plus_1, dtype=torch.float32).permute(1, 0)  # (6, 500)\n",
        "\n",
        "        return input_t, target_t_plus_1\n"
      ],
      "metadata": {
        "id": "AQ4Rn6rQD_WS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "data = TimeSeriesPerInstanceDataset(images)\n",
        "dataloader = DataLoader(data, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L9O1n_AECvg",
        "outputId": "d6e9424c-3d08-4633-a5b9-16b7664fc235"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
        "import keras, math\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "BxT1MfYRZFd_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Forward(keras.Model):\n",
        "    \"\"\"\n",
        "    An eight-layer residual convolutional network with dilated convolutions. Decreases the spatial dimensions by a factor of 8.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        block = lambda filters: keras.Sequential([\n",
        "            keras.layers.Dropout(.1),\n",
        "            keras.layers.Conv2D(filters, (3, 3), padding=\"same\"),\n",
        "            keras.layers.Activation(\"silu\"),\n",
        "            keras.layers.Conv2D(filters, (3, 3), dilation_rate=(2,2), padding=\"same\"),\n",
        "            keras.layers.GroupNormalization(groups=-1),\n",
        "            keras.layers.Activation(\"silu\")\n",
        "        ])\n",
        "        self.scaler = keras.layers.Normalization()\n",
        "        self.conv1x1_1 = keras.layers.Conv2D(32, (1, 1), padding=\"same\")\n",
        "        self.block1 = block(32)\n",
        "        self.pool1 = keras.layers.MaxPooling2D((1, 2))\n",
        "        self.conv1x1_2 = keras.layers.Conv2D(64, (1, 1), padding=\"same\")\n",
        "        self.block2 = block(64)\n",
        "        self.pool2 = keras.layers.MaxPooling2D((1, 2))\n",
        "        self.conv1x1_3 = keras.layers.Conv2D(128, (1, 1), padding=\"same\")\n",
        "        self.block3 = block(128)\n",
        "        self.pool3 = keras.layers.MaxPooling2D((1, 2))\n",
        "        self.block4 = block(256)\n",
        "\n",
        "    def call(self, input):\n",
        "        # Block 1 with residual connection\n",
        "        print(f\"Input shape before conv1x1_1: {input.shape}\")\n",
        "        h_ = self.conv1x1_1(input)\n",
        "        print(f\"Shape after conv1x1_1: {h_.shape}\")\n",
        "\n",
        "        h = self.block1(input)\n",
        "        print(f\"Shape after block1: {h.shape}\")\n",
        "\n",
        "        h = keras.layers.add([h, h_])  # Residual connection\n",
        "        print(f\"Shape after residual connection 1: {h.shape}\")\n",
        "\n",
        "        h = self.pool1(h)\n",
        "        print(f\"Shape after pool1: {h.shape}\")  # Check if this reduces the spatial dimensions too much\n",
        "\n",
        "        # Block 2 with residual connection\n",
        "        h_ = self.conv1x1_2(h)\n",
        "        print(f\"Shape after conv1x1_2: {h_.shape}\")\n",
        "\n",
        "        h = self.block2(h)\n",
        "        print(f\"Shape after block2: {h.shape}\")\n",
        "\n",
        "        h = keras.layers.add([h, h_])\n",
        "        print(f\"Shape after residual connection 2: {h.shape}\")\n",
        "\n",
        "        h = self.pool2(h)\n",
        "        print(f\"Shape after pool2: {h.shape}\")  # Check here as well\n",
        "\n",
        "        # Block 3 with residual connection\n",
        "        h_ = self.conv1x1_3(h)\n",
        "        print(f\"Shape after conv1x1_3: {h_.shape}\")\n",
        "\n",
        "        h = self.block3(h)\n",
        "        print(f\"Shape after block3: {h.shape}\")\n",
        "\n",
        "        h = keras.layers.add([h, h_])\n",
        "        print(f\"Shape after residual connection 3: {h.shape}\")\n",
        "\n",
        "        # Skip pool3 if height == 1 to avoid shrinking to 0\n",
        "        if h.shape[2] > 1:\n",
        "            h = self.pool3(h)\n",
        "\n",
        "\n",
        "        # Final block\n",
        "        h_t = self.block4(h)\n",
        "        print(f\"Shape after block4: {h_t.shape}\")\n",
        "\n",
        "        return h_t\n"
      ],
      "metadata": {
        "id": "Ma2TwYSKEII6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        block = lambda filters, activation=\"silu\", strides=None: keras.Sequential([\n",
        "            keras.layers.Dropout(.1),\n",
        "            keras.layers.Conv2DTranspose(filters, (3, 3), padding=\"same\"),\n",
        "            keras.layers.Activation(\"silu\"),\n",
        "            keras.layers.Conv2DTranspose(filters, (3, 3), strides=strides if strides else (1, 2), padding=\"same\"),\n",
        "            keras.layers.GroupNormalization(groups=-1),\n",
        "            keras.layers.Activation(activation)\n",
        "        ])\n",
        "        self.conv1x1_1 = keras.layers.Conv2DTranspose(256, (1, 1), strides=(1, 2), padding=\"same\")\n",
        "        self.block1 = block(256)\n",
        "        self.padding = keras.layers.ZeroPadding2D(((0, 0), (1, 0)))\n",
        "        self.conv1x1_2 = keras.layers.Conv2DTranspose(128, (1, 1), strides=(1, 2), padding=\"same\")\n",
        "        self.block2 = block(128)\n",
        "        self.conv1x1_3 = keras.layers.Conv2DTranspose(64, (1, 1), strides=(1, 2), padding=\"same\")\n",
        "        self.block3 = block(64)\n",
        "        self.block4 = block(6, \"linear\", (1, 1))\n",
        "\n",
        "    def call(self, x):\n",
        "        # x = keras.layers.concatenate([z, h_t])\n",
        "\n",
        "        x_ = self.conv1x1_1(x)\n",
        "        x = self.block1(x)\n",
        "        x = keras.layers.add([x, x_]) # residual connection\n",
        "\n",
        "        x = self.padding(x) # Required for exact shape matching\n",
        "        x_ = self.conv1x1_2(x)\n",
        "        x = self.block2(x)\n",
        "        x = keras.layers.add([x, x_])\n",
        "\n",
        "        x_ = self.conv1x1_3(x)\n",
        "        x = self.block3(x)\n",
        "        x = keras.layers.add([x, x_])\n",
        "\n",
        "        x_t_plus1_hat = self.block4(x)\n",
        "        return x_t_plus1_hat\n",
        "\n",
        "    @staticmethod\n",
        "    def log_bernoulli(x, p):\n",
        "        eps = 1.e-5\n",
        "        pp = keras.ops.clip(p, eps, 1. - eps)\n",
        "        log_p = x * keras.ops.log(pp) + (1. - x) * keras.ops.log(1. - pp)\n",
        "        return keras.ops.sum(log_p, list(range(1, keras.ops.ndim(x)))) # sum reduction\n",
        "\n",
        "    def log_prob(self, x_t_plus1, x_t_plus1_hat):\n",
        "        return self.log_bernoulli(x_t_plus1, keras.ops.sigmoid(x_t_plus1_hat))"
      ],
      "metadata": {
        "id": "Q__u98vmZC5K"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "\n",
        "class EncoderDecoderModel(keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(EncoderDecoderModel, self).__init__(**kwargs)\n",
        "        self.encoder = Forward()  # Instantiate the encoder (Forward in Keras)\n",
        "        self.decoder = Decoder()  # Instantiate the decoder (Decoder in Keras)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Pass the input through the encoder\n",
        "        encoded = self.encoder(x)\n",
        "        # Pass the encoded representation through the decoder\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded  # Return the final prediction\n"
      ],
      "metadata": {
        "id": "HC2q_693hJqT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model, send it to the device (GPU or CPU)\n",
        "model = EncoderDecoderModel()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39xxWGw8hmsx",
        "outputId": "a783a4e8-5304-454c-9fad-07717ede3af2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<EncoderDecoderModel name=encoder_decoder_model_6, built=False>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "\n",
        "num_epochs = 1000\n",
        "model = EncoderDecoderModel()\n",
        "optimizer = Adam(learning_rate=1e-4)\n",
        "criterion = MeanSquaredError()\n",
        "future_steps = 1  # Predicting 6 future time steps\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Iterate over batches of data\n",
        "    for step, (inputs, targets) in enumerate(dataloader):\n",
        "        # Reshape inputs if necessary (for Conv2D)\n",
        "        if len(inputs.shape) == 3:\n",
        "            inputs = tf.expand_dims(inputs, axis=2)  # Add height dimension (batch_size, width, height=1, channels)\n",
        "            inputs = tf.tile(inputs, [1, 1, 4, 1])  # Replicate height dimension\n",
        "            inputs = tf.transpose(inputs, perm=[0, 3, 2, 1])  # Adjust shape for Conv2D (batch_size, width, height, channels)\n",
        "\n",
        "        # Forward pass: get the model's prediction\n",
        "        outputs = model(inputs, training=True)  # Set training=True for training mode\n",
        "\n",
        "        # Adjust target shape for future time steps\n",
        "        targets = tf.reshape(targets, (batch_size, 500, future_steps, 6))\n",
        "\n",
        "        # Ensure the shapes are compatible before computing the loss\n",
        "        print(f\"Shape of outputs: {outputs.shape}\")\n",
        "        print(f\"Shape of targets: {targets.shape}\")\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(targets, outputs)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        running_loss += loss.numpy()\n",
        "\n",
        "    # Average loss over the epoch\n",
        "    epoch_loss = running_loss / (step + 1)\n",
        "\n",
        "    # Print loss after each epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xjqrQ0lEEI0i",
        "outputId": "4ad609a6-2729-47bc-dd3f-69386d6bc352"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape before conv1x1_1: torch.Size([8, 500, 4, 8])\n",
            "Shape after conv1x1_1: torch.Size([8, 500, 4, 32])\n",
            "Shape after block1: torch.Size([8, 500, 4, 32])\n",
            "Shape after residual connection 1: torch.Size([8, 500, 4, 32])\n",
            "Shape after pool1: torch.Size([8, 500, 2, 32])\n",
            "Shape after conv1x1_2: torch.Size([8, 500, 2, 64])\n",
            "Shape after block2: torch.Size([8, 500, 2, 64])\n",
            "Shape after residual connection 2: torch.Size([8, 500, 2, 64])\n",
            "Shape after pool2: torch.Size([8, 500, 1, 64])\n",
            "Shape after conv1x1_3: torch.Size([8, 500, 1, 128])\n",
            "Shape after block3: torch.Size([8, 500, 1, 128])\n",
            "Shape after residual connection 3: torch.Size([8, 500, 1, 128])\n",
            "Shape after block4: torch.Size([8, 500, 1, 256])\n",
            "Input shape before conv1x1_1: torch.Size([8, 500, 4, 8])\n",
            "Shape after conv1x1_1: torch.Size([8, 500, 4, 32])\n",
            "Shape after block1: torch.Size([8, 500, 4, 32])\n",
            "Shape after residual connection 1: torch.Size([8, 500, 4, 32])\n",
            "Shape after pool1: torch.Size([8, 500, 2, 32])\n",
            "Shape after conv1x1_2: torch.Size([8, 500, 2, 64])\n",
            "Shape after block2: torch.Size([8, 500, 2, 64])\n",
            "Shape after residual connection 2: torch.Size([8, 500, 2, 64])\n",
            "Shape after pool2: torch.Size([8, 500, 1, 64])\n",
            "Shape after conv1x1_3: torch.Size([8, 500, 1, 128])\n",
            "Shape after block3: torch.Size([8, 500, 1, 128])\n",
            "Shape after residual connection 3: torch.Size([8, 500, 1, 128])\n",
            "Shape after block4: torch.Size([8, 500, 1, 256])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'forward_10', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape before conv1x1_1: torch.Size([8, 500, 4, 8])\n",
            "Shape after conv1x1_1: torch.Size([8, 500, 4, 32])\n",
            "Shape after block1: torch.Size([8, 500, 4, 32])\n",
            "Shape after residual connection 1: torch.Size([8, 500, 4, 32])\n",
            "Shape after pool1: torch.Size([8, 500, 2, 32])\n",
            "Shape after conv1x1_2: torch.Size([8, 500, 2, 64])\n",
            "Shape after block2: torch.Size([8, 500, 2, 64])\n",
            "Shape after residual connection 2: torch.Size([8, 500, 2, 64])\n",
            "Shape after pool2: torch.Size([8, 500, 1, 64])\n",
            "Shape after conv1x1_3: torch.Size([8, 500, 1, 128])\n",
            "Shape after block3: torch.Size([8, 500, 1, 128])\n",
            "Shape after residual connection 3: torch.Size([8, 500, 1, 128])\n",
            "Shape after block4: torch.Size([8, 500, 1, 256])\n",
            "Shape of outputs: torch.Size([8, 500, 12, 6])\n",
            "Shape of targets: (8, 500, 1, 6)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot convert the argument `type_value`: tensor(1.1758, device='cuda:0', grad_fn=<DivBackward0>) to a TensorFlow DType.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-8207e3bbca67>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsTrainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         logging.vlog(\n\u001b[1;32m   1036\u001b[0m             \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The dtype of the target tensor must be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop_util.py\u001b[0m in \u001b[0;36mIsTrainable\u001b[0;34m(tensor_or_dtype)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_or_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m   trainable_dtypes = [dtypes.float16, dtypes.float32, dtypes.float64,\n\u001b[1;32m     61\u001b[0m                       \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_INTERN_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m   raise TypeError(f\"Cannot convert the argument `type_value`: {type_value!r} \"\n\u001b[0m\u001b[1;32m    853\u001b[0m                   \"to a TensorFlow DType.\")\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert the argument `type_value`: tensor(1.1758, device='cuda:0', grad_fn=<DivBackward0>) to a TensorFlow DType."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shape of targets: {targets.shape}\")\n",
        "print(f\"Shape of outputs: {outputs.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y64GFbKzsaIj",
        "outputId": "4ea19198-6349-4b2c-aefb-805f27c40648"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of targets: torch.Size([8, 6, 500])\n",
            "Shape of outputs: torch.Size([8, 500, 12, 6])\n"
          ]
        }
      ]
    }
  ]
}